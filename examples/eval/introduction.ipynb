{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "\n",
    "> 문서 작성일 : 2025.03.18\n",
    "\n",
    "GIP에서는 Model 및 prompt에 대한 평가(evaluation) 기능을 지원합니다. 이 문서는 GIP에서 제공하는 평가 기능을 사용하는 기본적인 방법에 대한 소개를 제공합니다.\n",
    "\n",
    "GIP의 평가 기능은 문서 작성일을 기준으로, 다음과 같은 단계로 구성되어 있습니다.\n",
    "\n",
    "1. Langfuse credential 연동\n",
    "2. Experiment 생성: 평가 대상 지정\n",
    "    - 현재는 Langfuse에 존재하는 데이터 소스에 대한 평가만 지원합니다\n",
    "        - [Dataset](https://api.reference.langfuse.com/?q=observation#tag/datasets/GET/api/public/v2/datasets/{datasetName})\n",
    "        - [Trace](https://api.reference.langfuse.com/?q=observation#tag/trace/GET/api/public/traces)\n",
    "        - [Observation](https://api.reference.langfuse.com/?q=observation#tag/observations/GET/api/public/observations)\n",
    "3. Evaluator 생성: 평가 방법 지정\n",
    "    - 현재는 LLM-as-a-Judge 방식의 평가만 지원합니다\n",
    "    - LLM-as-a-Judge 방식의 평가를 진행하기 위해서는 다음과 같은 정보를 필요로 합니다\n",
    "        - Judge model with parameters: 평가를 진행하기 위해 사용되는 LLM 모델 및 모델 파라미터 (`top_p`, `temperature`, `max_tokens`)\n",
    "        - Judge prompt: 평가 방법을 안내하는 프롬프트\n",
    "        - Metric: 평가 결과가 어떤 형식으로 제공되어야 하는지에 대한 정보\n",
    "            - 지원하는 Metric 유형\n",
    "                - Category: `bad` | `neutral` | `good`과 같이 사용자가 임의로 지정한 카테고리\n",
    "                - Numeric: `0.0` ~ `1.0` 과 같이 범위가 지정된 숫자\n",
    "                - Bool: `true` / `false`\n",
    "4. Experiment Run 생성: Experiment 및 evaluator를 이용한 평가 진행\n",
    "    - 평가 대상(experiment)에 대해서 어떻게 평가할지(evaluator)에 대한 정보의 조합으로 experiment run을 생성함으로서 **실제 평가를 진행합니다**\n",
    "    - 모델 성능 비교 등의 목적으로 평가를 진행하는 경우, `target_data_converter`를 사용하는 것이 도움이 될 수 있으며, 이에 관해서는 후술하겠습니다\n",
    "    - Experiment run에는 평가 진행 상황 및 결과가 포함되어 있으며, 평가가 완료된 경우에는 업로드 되어있는 상세 평가 결과를 받아볼 수 있습니다\n",
    "5. Auto Evaluation Config 생성: 자동 평가 설정\n",
    "    - 평가 대상(experiment)에 대해서 어떻게 평가할지(evaluator)에 대한 정보의 조합 및 자동 평가 실행 주기(schedule, timezone)을 지정하여 주기적인 **자동 평가를 진행하도록 구성합니다**\n",
    "    - 자동 평가 실행 주기(schedule)는 cron 표현식을 사용하여 지정할 수 있습니다\n",
    "    - 지정된 실행 주기에 맞춰서 experiment run을 생성하여 평가를 진행하기 때문에, experiment run 생성을 자동화하는 것으로 보셔도 됩니다."
   ],
   "id": "72381bbf8daf9034"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 사전 준비\n",
    "\n",
    "우선 다음의 python 패키지가 설치되어 있어야 합니다.\n",
    "\n",
    "- `requests`\n",
    "- `python-dotenv`\n",
    "\n",
    "환경 변수 파일(`.env`)를 사용하기 위해서는 `.env` 파일에 다음과 같은 내용을 채워주세요.\n",
    "\n",
    "```plaintext\n",
    "LANGFUSE_PUBLIC_KEY=pk-******\n",
    "LANGFUSE_SECRET_KEY=sk-******\n",
    "LANGFUSE_HOST=https://api.langfuse.com\n",
    "\n",
    "GIP_CONSOLE_HOST=https://dev-console-api.platform.a15t.com\n",
    "GIP_CONSOLE_ENTERPRISE_API_KEY=\n",
    "GIP_WORKSPACE_ID=\n",
    "```"
   ],
   "id": "de84d5e0ce763b4c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:32:50.877676Z",
     "start_time": "2025-03-20T11:32:50.871617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv() # Optional: 환경 변수 파일(.env)을 불러옵니다\n",
    "\n",
    "\n",
    "GIP_CONSOLE_HOST = os.getenv('GIP_CONSOLE_HOST', 'https://dev-console-api.platform.a15t.com')\n",
    "GIP_CONSOLE_ENTERPRISE_API_KEY = os.getenv('GIP_CONSOLE_ENTERPRISE_API_KEY')\n",
    "GIP_WORKSPACE_ID = os.getenv('GIP_WORKSPACE_ID')\n",
    "\n",
    "LANGFUSE_HOST = os.getenv('LANGFUSE_HOST', 'https://api.langfuse.com')\n",
    "LANGFUSE_PUBLIC_KEY = os.getenv('LANGFUSE_PUBLIC_KEY')\n",
    "LANGFUSE_SECRET_KEY = os.getenv('LANGFUSE_SECRET_KEY')"
   ],
   "id": "9a27d54e5c4eade7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 1\n",
      "Python-dotenv could not parse statement starting at line 5\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 인증 방법 관련 안내\n",
    "\n",
    "GIP Console의 API를 호출하는 데에는 두가지 방법이 있습니다.\n",
    "\n",
    "1. Enterprise API Key 사용\n",
    "2. GIP Console JWT Token 사용\n",
    "\n",
    "이 문서는 enterprise API key를 사용하는 것을 기준으로 evaluation API를 활용하는 방법에 대해서 안내하고 있으며, 필요하신 경우 인증 헤더 정보를 GIP Console JWT Token을 사용하도록 변경하시면 됩니다.\n",
    "\n",
    "#### 1. Enterprise API Key 사용"
   ],
   "id": "33ddb352ccafd5db"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:34:25.221544Z",
     "start_time": "2025-03-20T11:34:25.102248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "response = requests.get(\n",
    "    url=f'{GIP_CONSOLE_HOST}/api/workspaces/{GIP_WORKSPACE_ID}/models',\n",
    "    headers={'X-Gipc-Api-Key': GIP_CONSOLE_ENTERPRISE_API_KEY},\n",
    ")\n",
    "\n",
    "response.raise_for_status()"
   ],
   "id": "c87ae0a303b48130",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2. GIP Console JWT Token 사용",
   "id": "35299183e8c61ab3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:51:24.402914Z",
     "start_time": "2025-03-20T11:51:23.921738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "# GIP Console 로그인을 통해서 JWT Token을 발급받는 과정\n",
    "response = requests.post(\n",
    "    url='https://cognito-idp.ap-northeast-2.amazonaws.com',\n",
    "    headers={\n",
    "        'Content-Type': 'application/x-amz-json-1.1',\n",
    "        'X-Amz-Target': 'AWSCognitoIdentityProviderService.InitiateAuth',\n",
    "    },\n",
    "    json={\n",
    "        'AuthParameters': {\n",
    "            # GIP Console 로그인 시에 사용하는 계정 정보 기재\n",
    "            'USERNAME': 'keep-secret',\n",
    "            'PASSWORD': 'keep-secret',\n",
    "        },\n",
    "        'AuthFlow': 'USER_PASSWORD_AUTH',\n",
    "        'ClientId': '4dm722bemutkce2500cck1bbk4', # dev 환경용 client ID -> prod 환경의 GIP Console API 호출시에는 별도 문의 필요\n",
    "    },\n",
    ")\n",
    "\n",
    "response.raise_for_status()\n",
    "\n",
    "auth_response = response.json()\n",
    "id_token = auth_response['AuthenticationResult']['IdToken']\n",
    "\n",
    "# GIP Console 로그인을 통해서 얻은 JWT Token을 사용해서 GIP Console API 호출\n",
    "response = requests.get(\n",
    "    url=f'{GIP_CONSOLE_HOST}/api/workspaces/{GIP_WORKSPACE_ID}/models',\n",
    "    headers={'Authorization': f'Bearer {id_token}'}\n",
    ")\n",
    "\n",
    "response.raise_for_status()"
   ],
   "id": "52367b66f85d9870",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Langfuse credential 연동\n",
    "\n",
    "현재 GIP에서는 Langfuse에 존재하는 데이터 소스에 대한 평가만 우선적으로 지원하기 때문에, Langfuse API key 연동이 필요합니다.\n",
    "\n",
    "Langfuse의 API key는 각 프로젝트 별로 설정되며, 다음의 문서를 통해서 API key 생성 / 조회 방법을 확인하실 수 있습니다.\n",
    "\n",
    "[Where are my Langfuse API keys?](https://langfuse.com/faq/all/where-are-langfuse-api-keys)\n",
    "\n",
    "Langfuse API key를 확인한 이후에는 다음의 과정을 거쳐서 GIP에 연동하실 수 있습니다."
   ],
   "id": "3457175019641f20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T10:18:41.495479Z",
     "start_time": "2025-03-18T10:18:41.296510Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integrated service credential ID:  ccd1ea20-5d1c-47e8-9a4b-a36ced7cb856\n"
     ]
    }
   ],
   "execution_count": 6,
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "response = requests.post(\n",
    "    url=f'{GIP_CONSOLE_HOST}/api/workspaces/{GIP_WORKSPACE_ID}/integrated-services/credentials',\n",
    "    headers={'X-Gipc-Api-Key': GIP_CONSOLE_ENTERPRISE_API_KEY},\n",
    "    json={\n",
    "        'service_name': 'LANGFUSE',\n",
    "        'alias': 'gip-eval-intro-demo', # GIP <-> Langfuse API Key 연동시의 별칭 (Langfuse project name 등으로 설정하시면 편합니다)\n",
    "        'base_url': LANGFUSE_HOST,\n",
    "        'public_key': LANGFUSE_PUBLIC_KEY,\n",
    "        'private_key': LANGFUSE_SECRET_KEY,\n",
    "    },\n",
    ")\n",
    "\n",
    "response.raise_for_status()\n",
    "\n",
    "integrated_service_credential = response.json()\n",
    "integrated_service_credential_id = integrated_service_credential['id']\n",
    "\n",
    "print('Integrated service credential ID: ', integrated_service_credential_id)"
   ],
   "id": "8f138892e4d93984"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Experiment 생성\n",
    "\n",
    "Experiment는 평가 대상에 대한 정보를 갖고 있습니다. 현재는 Langfuse에 존재하는 데이터 소스에 대한 평가만 지원하고 있으며, 다음의 데이터 소스에 대한 평가를 지원합니다.\n",
    "\n",
    "- [Dataset](https://api.reference.langfuse.com/?q=observation#tag/datasets/GET/api/public/v2/datasets/{datasetName})\n",
    "- [Trace](https://api.reference.langfuse.com/?q=observation#tag/trace/GET/api/public/traces)\n",
    "- [Observation](https://api.reference.langfuse.com/?q=observation#tag/observations/GET/api/public/observations)\n",
    "\n",
    "각각의 데이터 소스에 대해서 experiment를 생성하는 방법은 다음과 같습니다.\n",
    "\n",
    "### 2.1 Dataset"
   ],
   "id": "d1f03193a43211e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T10:29:32.123655Z",
     "start_time": "2025-03-18T10:29:31.898992Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment ID (dataset):  0195a8cd-2752-7e32-a822-5a92ad7d81dd\n"
     ]
    }
   ],
   "execution_count": 10,
   "source": [
    "response = requests.post(\n",
    "    url=f'{GIP_CONSOLE_HOST}/api/workspaces/{GIP_WORKSPACE_ID}/evals/experiments',\n",
    "    headers={'X-Gipc-Api-Key': GIP_CONSOLE_ENTERPRISE_API_KEY},\n",
    "    json={\n",
    "        'name': 'Langfuse dataset > `JSONL_samples-plan_suggestion_dataset_v2`',\n",
    "        'description': 'GIP evaluation introduction demo - dataset 평가 대상 지정', # Optional\n",
    "        'type': 'langfuse_dataset',\n",
    "        'config': {\n",
    "            'integrated_service_credential_id': integrated_service_credential_id,\n",
    "            'dataset_name': 'JSONL_samples-plan_suggestion_dataset_v2', # 평가 대상이 되는 Langfuse dataset 이름 (WARN: 현재 GIP에서는 dataset 이름에 공백이 없는 경우만 지원합니다)\n",
    "            'include_archived_items': False, # Dataset 내의 아카이브된 데이터 포함 여부\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "response.raise_for_status()\n",
    "\n",
    "dataset_experiment = response.json()\n",
    "dataset_experiment_id = dataset_experiment['id']\n",
    "\n",
    "print('Experiment ID (dataset): ', dataset_experiment_id)"
   ],
   "id": "8363674d5ee61a2c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.2 Trace",
   "id": "58cc881e0d950d67"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T10:29:44.074832Z",
     "start_time": "2025-03-18T10:29:43.882957Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment ID (trace):  0195a8cd-5603-70c2-86ef-412d8bb21451\n"
     ]
    }
   ],
   "execution_count": 11,
   "source": [
    "response = requests.post(\n",
    "    url=f'{GIP_CONSOLE_HOST}/api/workspaces/{GIP_WORKSPACE_ID}/evals/experiments',\n",
    "    headers={'X-Gipc-Api-Key': GIP_CONSOLE_ENTERPRISE_API_KEY},\n",
    "    json={\n",
    "        'name': 'Langfuse trace',\n",
    "        'description': 'GIP evaluation introduction demo - trace 평가 대상 지정', # Optional\n",
    "        'type': 'langfuse_trace',\n",
    "        'config': {\n",
    "            'integrated_service_credential_id': integrated_service_credential_id,\n",
    "            'sampling_rate': 1.0, # 0.0 ~ 1.0 사이의 값으로 설정\n",
    "            'max_trace_count': 10, # 추출할 trace의 최대 개수\n",
    "            'trace_filter_request': { # Trace 필터링 조건 - Langfuse API Reference 참조: https://api.reference.langfuse.com/#tag/trace/GET/api/public/traces\n",
    "                'name': None,\n",
    "                'from_timestamp': None,\n",
    "                'to_timestamp': 1742293735000,\n",
    "                'tags': None,\n",
    "                'version': None,\n",
    "                'release': None,\n",
    "                'relative_time_range': None,\n",
    "            }\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "response.raise_for_status()\n",
    "\n",
    "trace_experiment = response.json()\n",
    "trace_experiment_id = trace_experiment['id']\n",
    "\n",
    "print('Experiment ID (trace): ', trace_experiment_id)"
   ],
   "id": "9bf81d44a1b3cecd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.3 Observation",
   "id": "b0b1b2de825d797f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "response = requests.post(\n",
    "    url=f'{GIP_CONSOLE_HOST}/api/workspaces/{GIP_WORKSPACE_ID}/evals/experiments',\n",
    "    headers={'X-Gipc-Api-Key': GIP_CONSOLE_ENTERPRISE_API_KEY},\n",
    "    json={\n",
    "        'name': 'Langfuse observation',\n",
    "        'description': 'GIP evaluation introduction demo - observation 평가 대상 지정', # Optional\n",
    "        'type': 'langfuse_observation',\n",
    "        'config': {\n",
    "            'integrated_service_credential_id': integrated_service_credential_id,\n",
    "            'sampling_rate': 1.0, # 0.0 ~ 1.0 사이의 값으로 설정\n",
    "            'max_observation_count': 10, # 추출할 observation의 최대 개수\n",
    "            'observation_filter_request': { # Observation 필터링 조건 - Langfuse API Reference 참조: https://api.reference.langfuse.com/#tag/observations/GET/api/public/observations\n",
    "                'name': None,\n",
    "                'from_start_time': None,\n",
    "                'to_start_time': 1742293735000,\n",
    "                'version': None,\n",
    "                'relative_time_range': None,\n",
    "            }\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "response.raise_for_status()\n",
    "\n",
    "observation_experiment = response.json()\n",
    "observation_experiment_id = observation_experiment['id']\n",
    "\n",
    "print('Experiment ID (observation): ', observation_experiment_id)"
   ],
   "id": "55af1f89fe38462f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "experiment의 `type` 값이 `langfuse_observation` 혹은 `langfuse_trace`인 경우 다음의 필드를 활용하여 고정 시간 범위 내의 데이터를 조회할 수 있습니다.\n",
    "\n",
    "- `observation_filter_request.from_start_time`\n",
    "- `observation_filter_request.to_start_time`\n",
    "- `trace_filter_request.from_timestamp`\n",
    "- `trace_filter_request.to_timestamp`\n",
    "\n",
    "이는 experiment run 혹은 auto evaluation config를 통한 평가를 진행할때, 평가 진행 시점을 기준으로 한 상대적인 데이터를 조회하는 데에 제약사항이 되며, GIP에서는 이러한 제약사항을 해결하기 위해 `relative_time_range`를 지원합니다.\n",
    "\n",
    "`relative_time_range` 필드를 활용하면 다음의 필드 값을 무시하게 되며, `relative_time_range`가 최우선시 되어 적용됩니다.\n",
    "\n",
    "- `observation_filter_request.from_start_time`\n",
    "- `observation_filter_request.to_start_time`\n",
    "- `trace_filter_request.from_timestamp`\n",
    "- `trace_filter_request.to_timestamp`\n",
    "\n",
    "`relative_time_range` 필드 값은 [ISO-8601의 duration 형식](https://en.wikipedia.org/wiki/ISO_8601#Durations)을 따르며, 최근 12시간 동안의 데이터를 조회하고 싶은 경우 `PT12H`로 기재해서 사용하시면 됩니다.\n",
    "\n",
    "### Trace 적용 예시"
   ],
   "id": "8c73393d87ec4a9d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "response = requests.post(\n",
    "    url=f'{GIP_CONSOLE_HOST}/api/workspaces/{GIP_WORKSPACE_ID}/evals/experiments',\n",
    "    headers={'X-Gipc-Api-Key': GIP_CONSOLE_ENTERPRISE_API_KEY},\n",
    "    json={\n",
    "        'name': 'Langfuse trace + `relative_time_range`',\n",
    "        'description': 'GIP evaluation introduction demo - trace 평가 대상 지정 w/ relative time range', # Optional\n",
    "        'type': 'langfuse_trace',\n",
    "        'config': {\n",
    "            'integrated_service_credential_id': integrated_service_credential_id,\n",
    "            'sampling_rate': 1.0,\n",
    "            'max_trace_count': 10,\n",
    "            'trace_filter_request': {\n",
    "                'name': None,\n",
    "                'from_timestamp': None,\n",
    "                'to_timestamp': None,\n",
    "                'tags': None,\n",
    "                'version': None,\n",
    "                'release': None,\n",
    "                'relative_time_range': 'PT12H', # NOTE: 최근 12시간 동안의 데이터\n",
    "            }\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "response.raise_for_status()"
   ],
   "id": "34849404ece169d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Observation 적용 예시",
   "id": "dee0f89903becec4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "response = requests.post(\n",
    "    url=f'{GIP_CONSOLE_HOST}/api/workspaces/{GIP_WORKSPACE_ID}/evals/experiments',\n",
    "    headers={'X-Gipc-Api-Key': GIP_CONSOLE_ENTERPRISE_API_KEY},\n",
    "    json={\n",
    "        'name': 'Langfuse observation + `relative_time_range`',\n",
    "        'description': 'GIP evaluation introduction demo - observation 평가 대상 지정 w/ relative time range', # Optional\n",
    "        'type': 'langfuse_observation',\n",
    "        'config': {\n",
    "            'integrated_service_credential_id': integrated_service_credential_id,\n",
    "            'sampling_rate': 1.0,\n",
    "            'max_observation_count': 10,\n",
    "            'observation_filter_request': {\n",
    "                'name': None,\n",
    "                'from_start_time': None,\n",
    "                'to_start_time': None,\n",
    "                'version': None,\n",
    "                'relative_time_range': 'PT24H', # NOTE: 최근 24시간 동안의 데이터\n",
    "            }\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "response.raise_for_status()"
   ],
   "id": "cd637f5411299dbf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Evaluator 생성\n",
    "\n",
    "Evaluator는 평가 방법에 대한 정보를 갖고 있습니다. 현재는 LLM-as-a-Judge 방식의 평가만 지원하고 있으며, LLM-as-a-Judge 방식의 평가를 진행하기 위해서는 다음과 같은 정보를 필요로 합니다.\n",
    "\n",
    "- Judge model with parameters: 평가를 진행하기 위해 사용되는 LLM 모델 및 모델 파라미터 (`top_p`, `temperature`, `max_tokens`)\n",
    "- Judge prompt: 평가 방법을 안내하는 프롬프트\n",
    "- Metric: 평가 결과가 어떤 형식으로 제공되어야 하는지에 대한 정보\n",
    "\n",
    "Judge prompt의 예시는 다음과 같습니다:\n",
    "\n",
    "<details>\n",
    "<summary>Judge prompt example</summary>\n",
    "<p>\n",
    "\n",
    "```markdown\n",
    "You are an impartial AI judge. Your task is to evaluate whether the given **output** is correct based on the provided **input**.\n",
    "\n",
    "## Evaluation Criteria\n",
    "**Accuracy**: Does the output provide correct?\n",
    "**Explanation:** Does the output provide an explanation or justification for its statements?\n",
    "\n",
    "## Instructions for Evaluation\n",
    "1. Carefully review the input and corresponding output.\n",
    "2. Assess the output based on the criteria above.\n",
    "3. Provide a detailed explanation for each criterion, noting strengths and weaknesses.\n",
    "4. Assign an overall accuracy rating: `correct` (highly accurate) or `incorrect` (highly inaccurate).\n",
    "\n",
    "## Answer Format\n",
    "\n",
    "**Evaluation Rationale:**\n",
    "<rationale>\n",
    "Provide a detailed explanation for each criterion.\n",
    "</rationale>\n",
    "\n",
    "**Overall Accuracy Rating:**\n",
    "<rating>\n",
    "Provide an overall accuracy rating here based on the evaluation rationale. Follow the evaluation instructions.\n",
    "</rating>\n",
    "\n",
    "## Example Evaluation\n",
    "\n",
    "**Input:** `[ { 'role': 'user', 'content': 'Is a fox a member of the dog family?' } ]`\n",
    "\n",
    "\n",
    "**Output:** `{ 'role': 'assistant', 'content': 'Yes' }`\n",
    "\n",
    "**Evaluation Rationale:**\n",
    "<rationale>\n",
    "- **Accuracy**: Correct. A typical fox is a member of the dog family.\n",
    "- **Explanation:** The output does not provide an explanation, but the answer is concise and accurate.\n",
    "</rationale>\n",
    "\n",
    "**Overall Accuracy Rating:**\n",
    "<rating>\n",
    "correct\n",
    "</rating>\n",
    "\n",
    "Let’s think step by step.\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "현재 지원되는 metric의 유형은 다음과 같습니다.\n",
    "\n",
    "- Category: `bad` | `neutral` | `good`과 같이 사용자가 임의로 지정한 카테고리\n",
    "- Numeric: `0.0` ~ `1.0` 과 같이 범위가 지정된 숫자\n",
    "- Bool: `true` / `false`\n",
    "\n",
    "각각의 metric 유형에 대해서 evaluator를 생성하는 방법은 다음과 같습니다.\n",
    "\n",
    "### 3.1 Category Metric\n"
   ],
   "id": "fb27838564377721"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T11:15:23.350609Z",
     "start_time": "2025-03-18T11:15:23.225833Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluator ID (category metric):  0195a8f7-224e-7163-8feb-12c4ca1ae5ad\n"
     ]
    }
   ],
   "execution_count": 16,
   "source": [
    "judge_prompt = '''\n",
    "You are an impartial AI judge. Your task is to evaluate whether the given **output** is correct based on the provided **input**.\n",
    "\n",
    "## Evaluation Criteria\n",
    "**Accuracy**: Does the output provide correct?\n",
    "**Explanation:** Does the output provide an explanation or justification for its statements?\n",
    "\n",
    "## Instructions for Evaluation\n",
    "1. Carefully review the input and corresponding output.\n",
    "2. Assess the output based on the criteria above.\n",
    "3. Provide a detailed explanation for each criterion, noting strengths and weaknesses.\n",
    "4. Assign an overall accuracy rating: `correct` (highly accurate) or `incorrect` (highly inaccurate).\n",
    "\n",
    "## Answer Format\n",
    "\n",
    "**Evaluation Rationale:**\n",
    "<rationale>\n",
    "Provide a detailed explanation for each criterion.\n",
    "</rationale>\n",
    "\n",
    "**Overall Accuracy Rating:**\n",
    "<rating>\n",
    "Provide an overall accuracy rating here based on the evaluation rationale. Follow the evaluation instructions.\n",
    "</rating>\n",
    "\n",
    "## Example Evaluation\n",
    "\n",
    "**Input:**\n",
    "```json\n",
    "[ { 'role': 'user', 'content': 'Is a fox a member of the dog family?' } ]\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```json\n",
    "{ 'role': 'assistant', 'content': 'Yes' }\n",
    "```\n",
    "\n",
    "**Evaluation Rationale:**\n",
    "<rationale>\n",
    "- **Accuracy**: Correct. A typical fox is a member of the dog family.\n",
    "- **Explanation:** The output does not provide an explanation, but the answer is concise and accurate.\n",
    "</rationale>\n",
    "\n",
    "**Overall Accuracy Rating:**\n",
    "<rating>\n",
    "correct\n",
    "</rating>\n",
    "\n",
    "Let’s think step by step.\n",
    "'''.strip()\n",
    "\n",
    "response = requests.post(\n",
    "    url=f'{GIP_CONSOLE_HOST}/api/workspaces/{GIP_WORKSPACE_ID}/evals/evaluators',\n",
    "    headers={'X-Gipc-Api-Key': GIP_CONSOLE_ENTERPRISE_API_KEY},\n",
    "    json={\n",
    "        'name': 'Category Metric Evaluator',\n",
    "        'metric': {\n",
    "            'type': 'category',\n",
    "            'config': [\n",
    "                {'index': 0, 'name':  'bad'},\n",
    "                {'index': 1, 'name':  'neutral'},\n",
    "                {'index': 2, 'name':  'good'},\n",
    "            ],\n",
    "        },\n",
    "        'type': 'llm_judge',\n",
    "        'evaluation_config': {\n",
    "            'judge_model_seq_id': 19,\n",
    "            'judge_model_public_id': 'openai/gpt-4',\n",
    "            'judge_model_parameters': {\n",
    "                'top_p': 0.5,\n",
    "                'temperature': 0.5,\n",
    "                'max_tokens': 50,\n",
    "            },\n",
    "            'judge_prompt': judge_prompt,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "response.raise_for_status()\n",
    "\n",
    "category_metric_evaluator = response.json()\n",
    "category_metric_evaluator_id = category_metric_evaluator['id']\n",
    "\n",
    "print('Evaluator ID (category metric): ', category_metric_evaluator_id)"
   ],
   "id": "5f82ea4a2ba12644"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.2 Numeric Metric",
   "id": "fafc492de41de0f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T11:16:01.980892Z",
     "start_time": "2025-03-18T11:16:01.858169Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluator ID (numeric metric):  0195a8f7-b924-7b90-9484-477ea0863b82\n"
     ]
    }
   ],
   "execution_count": 17,
   "source": [
    "judge_prompt = '''\n",
    "You are an impartial AI judge. Your task is to evaluate whether the given **output** is correct based on the provided **input**.\n",
    "\n",
    "## Evaluation Criteria\n",
    "**Factual Accuracy:** Are the facts, figures, and information presented in the output accurate and verifiable?\n",
    "**Logical Consistency:** Does the output follow a logical sequence and make sense in the context of the input? Are there any logical fallacies or inconsistencies?\n",
    "**Adherence to Requirements:** Does the output meet the specific requirements or criteria outlined in the input? Are all necessary conditions and constraints satisfied?\n",
    "**Precision:** Is the information in the output precise and specific? Are there any vague or ambiguous statements that could lead to misinterpretation?\n",
    "**Error-Free:** Is the output free from typographical, grammatical, or computational errors that could affect its correctness?\n",
    "\n",
    "## Instructions for Evaluation\n",
    "1. Carefully review the input and corresponding output.\n",
    "2. Assess the output based on the criteria above.\n",
    "3. Provide a detailed explanation for each criterion, noting strengths and weaknesses.\n",
    "4. Assign an overall accuracy rating between `1.0` (highly accurate) and `0.0` (highly inaccurate).\n",
    "\n",
    "## Answer Format\n",
    "\n",
    "**Evaluation Rationale:**\n",
    "<rationale>\n",
    "Provide a detailed explanation for each criterion.\n",
    "</rationale>\n",
    "\n",
    "**Overall Accuracy Rating:**\n",
    "<rating>\n",
    "Provide an overall accuracy rating here based on the evaluation rationale. Follow the evaluation instructions.\n",
    "</rating>\n",
    "\n",
    "## Example Evaluation\n",
    "\n",
    "**Input:**\n",
    "How many legs does a cat have?\n",
    "\n",
    "**Output:**\n",
    "2\n",
    "\n",
    "**Evaluation Rationale:**\n",
    "<rationale>\n",
    "- **Factual Accuracy:** Incorrect. A typical cat has 4 legs, not 2.\n",
    "- **Logical Consistency:** The output does not logically follow from the input.\n",
    "- **Adherence to Requirements:** The output fails to provide the correct number of legs.\n",
    "- **Precision:** While specific, the number '2' is incorrect.\n",
    "- **Error-Free:** No grammatical errors, but the factual inaccuracy affects correctness.\n",
    "</rationale>\n",
    "\n",
    "**Overall Accuracy Rating:**\n",
    "<rating>\n",
    "0.0\n",
    "</rating>\n",
    "\n",
    "Let’s think step by step.\n",
    "'''.strip()\n",
    "\n",
    "response = requests.post(\n",
    "    url=f'{GIP_CONSOLE_HOST}/api/workspaces/{GIP_WORKSPACE_ID}/evals/evaluators',\n",
    "    headers={'X-Gipc-Api-Key': GIP_CONSOLE_ENTERPRISE_API_KEY},\n",
    "    json={\n",
    "        'name': 'Numeric Metric Evaluator',\n",
    "        'metric': {\n",
    "            'type': 'numeric',\n",
    "            'config': {\n",
    "                'min': {\n",
    "                    'value': 0.0,\n",
    "                },\n",
    "                'max': {\n",
    "                    'value': 1.0,\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "        'type': 'llm_judge',\n",
    "        'evaluation_config': {\n",
    "            'judge_model_seq_id': 19,\n",
    "            'judge_model_public_id': 'openai/gpt-4',\n",
    "            'judge_model_parameters': {\n",
    "                'top_p': 0.5,\n",
    "                'temperature': 0.5,\n",
    "                'max_tokens': 50,\n",
    "            },\n",
    "            'judge_prompt': judge_prompt,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "response.raise_for_status()\n",
    "\n",
    "numeric_metric_evaluator = response.json()\n",
    "numeric_metric_evaluator_id = numeric_metric_evaluator['id']\n",
    "\n",
    "print('Evaluator ID (numeric metric): ', numeric_metric_evaluator_id)"
   ],
   "id": "7be3c02eaeb649fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.3 Bool Metric",
   "id": "3cfafbbb2c1545a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T11:17:30.667625Z",
     "start_time": "2025-03-18T11:17:30.498491Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluator ID (bool metric):  0195a8f9-13a6-7851-8f81-a2e80509871a\n"
     ]
    }
   ],
   "execution_count": 18,
   "source": [
    "judge_prompt = '''\n",
    "You are an impartial AI judge. Your task is to evaluate whether the given **output** is correct based on the provided **input**.\n",
    "\n",
    "## Evaluation Criteria\n",
    "**Factual Accuracy:** Are the facts, figures, and information presented in the output accurate and verifiable?\n",
    "**Logical Consistency:** Does the output follow a logical sequence and make sense in the context of the input? Are there any logical fallacies or inconsistencies?\n",
    "**Adherence to Requirements:** Does the output meet the specific requirements or criteria outlined in the input? Are all necessary conditions and constraints satisfied?\n",
    "**Precision:** Is the information in the output precise and specific? Are there any vague or ambiguous statements that could lead to misinterpretation?\n",
    "**Error-Free:** Is the output free from typographical, grammatical, or computational errors that could affect its correctness?\n",
    "\n",
    "## Instructions for Evaluation\n",
    "1. Carefully review the input and corresponding output.\n",
    "2. Assess the output based on the criteria above.\n",
    "3. Provide a detailed explanation for each criterion, noting strengths and weaknesses.\n",
    "4. Assign an overall accuracy rating: `true` (highly accurate) or `false` (highly inaccurate).\n",
    "\n",
    "## Answer Format\n",
    "\n",
    "**Evaluation Rationale:**\n",
    "<rationale>\n",
    "Provide a detailed explanation for each criterion.\n",
    "</rationale>\n",
    "\n",
    "**Overall Accuracy Rating:**\n",
    "<rating>\n",
    "Provide an overall accuracy rating here based on the evaluation rationale. Follow the evaluation instructions.\n",
    "</rating>\n",
    "\n",
    "## Example Evaluation\n",
    "\n",
    "**Input:**\n",
    "How many legs does a cat have?\n",
    "\n",
    "**Output:**\n",
    "2\n",
    "\n",
    "**Evaluation Rationale:**\n",
    "<rationale>\n",
    "- **Factual Accuracy:** Incorrect. A typical cat has 4 legs, not 2.\n",
    "- **Logical Consistency:** The output does not logically follow from the input.\n",
    "- **Adherence to Requirements:** The output fails to provide the correct number of legs.\n",
    "- **Precision:** While specific, the number '2' is incorrect.\n",
    "- **Error-Free:** No grammatical errors, but the factual inaccuracy affects correctness.\n",
    "</rationale>\n",
    "\n",
    "**Overall Accuracy Rating:**\n",
    "<rating>\n",
    "false\n",
    "</rating>\n",
    "\n",
    "Let’s think step by step.\n",
    "'''.strip()\n",
    "\n",
    "response = requests.post(\n",
    "    url=f'{GIP_CONSOLE_HOST}/api/workspaces/{GIP_WORKSPACE_ID}/evals/evaluators',\n",
    "    headers={'X-Gipc-Api-Key': GIP_CONSOLE_ENTERPRISE_API_KEY},\n",
    "    json={\n",
    "        'name': 'Numeric Metric Evaluator',\n",
    "        'metric': {\n",
    "            'type': 'bool',\n",
    "        },\n",
    "        'type': 'llm_judge',\n",
    "        'evaluation_config': {\n",
    "            'judge_model_seq_id': 19,\n",
    "            'judge_model_public_id': 'openai/gpt-4',\n",
    "            'judge_model_parameters': {\n",
    "                'top_p': 0.5,\n",
    "                'temperature': 0.5,\n",
    "                'max_tokens': 50,\n",
    "            },\n",
    "            'judge_prompt': judge_prompt,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "response.raise_for_status()\n",
    "\n",
    "bool_metric_evaluator = response.json()\n",
    "bool_metric_evaluator_id = bool_metric_evaluator['id']\n",
    "\n",
    "print('Evaluator ID (bool metric): ', bool_metric_evaluator_id)"
   ],
   "id": "71d63994140c9cf9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Experiment Run 생성\n",
    "\n",
    "Experiment와 evaluator의 조합으로 experiment run을 생성하면 평가가 진행됩니다. Experiment run은 평가 진행 상황 및 결과에 대한 정보를 포함합니다.\n",
    "\n",
    "기본적인 형태의 experiment run을 생성하는 과정은 다음과 같습니다."
   ],
   "id": "d98372a18eaec5d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T11:34:57.867613Z",
     "start_time": "2025-03-18T11:34:57.661414Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Run ID (basic):  0195a909-0e0b-75f1-99d9-78c9082a9210\n"
     ]
    }
   ],
   "execution_count": 19,
   "source": [
    "response = requests.post(\n",
    "    url=f'{GIP_CONSOLE_HOST}/api/workspaces/{GIP_WORKSPACE_ID}/evals/experiments/{dataset_experiment_id}/runs',\n",
    "    headers={'X-Gipc-Api-Key': GIP_CONSOLE_ENTERPRISE_API_KEY},\n",
    "    json={\n",
    "        'evaluator_id': category_metric_evaluator_id,\n",
    "        'display_name': 'Langfuse dataset + category metric experiment run',\n",
    "        'target_data_converter': None,\n",
    "        'metadata': None,\n",
    "    },\n",
    ")\n",
    "\n",
    "response.raise_for_status()\n",
    "\n",
    "basic_experiment_run = response.json()\n",
    "basic_experiment_run_id = basic_experiment_run['id']\n",
    "\n",
    "print('Experiment Run ID (basic): ', basic_experiment_run_id)"
   ],
   "id": "3145f81eea875ac9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "평가 진행 과정에서 다소 시간이 걸릴 수 있기 때문에, experiment run의 상태를 확인한 후에 상세 결과를 확인해보겠습니다.",
   "id": "28de5df02b0446de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T12:15:42.855298Z",
     "start_time": "2025-03-18T12:15:42.758249Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic experiment run: \n",
      "\tstatus:  DONE\n",
      "\taggregated_result:\n",
      "\t {'error': None, 'total_usage': {'prompt_tokens': 751295, 'completion_tokens': 26534, 'total_tokens': 777829, 'web_grounding_requests': 0, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'text_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0, 'text_tokens': 0}}}\n",
      "\tresult_path:  workspaces/2d6d2b89-8454-40a1-8d85-0aaba543f47e/experiment_runs/0195a909-0e0b-75f1-99d9-78c9082a9210.csv\n"
     ]
    }
   ],
   "execution_count": 36,
   "source": [
    "response = requests.get(\n",
    "    url=f'{GIP_CONSOLE_HOST}/api/workspaces/{GIP_WORKSPACE_ID}/evals/experiments/{dataset_experiment_id}/runs/{basic_experiment_run_id}',\n",
    "    headers={'X-Gipc-Api-Key': GIP_CONSOLE_ENTERPRISE_API_KEY},\n",
    ")\n",
    "\n",
    "response.raise_for_status()\n",
    "\n",
    "basic_experiment_run = response.json()\n",
    "\n",
    "print('Basic experiment run: ')\n",
    "print('\\tstatus: ', basic_experiment_run['status'])\n",
    "print('\\taggregated_result:\\n\\t', basic_experiment_run['aggregated_result'])\n",
    "print('\\tresult_path: ', basic_experiment_run['result_path'])"
   ],
   "id": "326d87da87d8625d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "experiment run의 상태가 `DONE`이 되면, 상세 결과를 확인할 수 있습니다. 상세 결과는 다음과 같이 확인 가능합니다.",
   "id": "4dc04d49d37cbb89"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T12:18:28.669899Z",
     "start_time": "2025-03-18T12:18:28.036545Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 42,
   "source": [
    "import csv\n",
    "import io\n",
    "\n",
    "response = requests.get(\n",
    "    url=f'{GIP_CONSOLE_HOST}/api/workspaces/{GIP_WORKSPACE_ID}/evals/experiments/{dataset_experiment_id}/runs/{basic_experiment_run_id}/download',\n",
    "    headers={'X-Gipc-Api-Key': GIP_CONSOLE_ENTERPRISE_API_KEY},\n",
    ")\n",
    "\n",
    "response.raise_for_status()\n",
    "\n",
    "csv_content = response.content.decode('utf-8')\n",
    "csv_reader = csv.DictReader(io.StringIO(csv_content))\n",
    "\n",
    "for index, row in enumerate(csv_reader):\n",
    "    if index > 5: # NOTE: 상세 결과의 일부만 확인\n",
    "        break\n",
    "\n",
    "    print('Row #', index)\n",
    "    print('\\texperiment input:\\n\\t\\t', row['input_data_from_experiment'])\n",
    "    print('\\texperiment run input:\\n\\t\\t', row['input_data_from_experiment_run'])\n",
    "    print('\\texperiment output:\\n\\t\\t', row['output_data_from_experiment'])\n",
    "    print('\\texperiment run output:\\n\\t\\t', row['output_data_from_experiment_run'])\n",
    "    print('\\tpassed:', row['passed'])\n",
    "    print('\\terror_message:', row['error_message'] or 'x') # NOTE: `passed` 값이 `false`인 경우에 어떤 문제가 있었는지에 대해 나타냄\n",
    "    print('\\tscore:', row['score'])\n",
    "    print('\\trationale:\\n\\t\\t', row['rationale'])"
   ],
   "id": "f6276565a9e6cbbc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "이렇게 기본적인 experiment run은 experiment에 지정된 데이터 소스로부터 input/output prompt를 가져온 후, evaluator에 지정된 judge prompt를 통한 평가를 진행하게 됩니다. 이는 input/output prompt에 대한 평가를 지원하는 것입니다.\n",
    "\n",
    "다만, model 별로 어떤 output이 나오는지에 대해서 평가하고 싶은 상황에 대해서는 기본적인 experiment run을 생성하는 것으로 부족합니다. 이런 경우에는 `target_data_converter`를 사용하여 평가 대상 데이터를 변환하는 과정을 추가할 수 있습니다.\n",
    "\n",
    "`target_data_converter`를 사용하게 되면, exeperiment에 지정된 데이터 소스로부터 input prompt를 가져온 후에 `target_data_converter`를 거쳐서 한번 더 input/output prompt가 가공됩니다. 평가 대상이 `target_data_converter`를 거친 가공된 input/output prompt가 되는 것입니다.\n",
    "\n",
    "자세한 사항은 다음의 예시를 통해 확인해보겠습니다."
   ],
   "id": "6b47ea5d15c0a031"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T12:40:55.846201Z",
     "start_time": "2025-03-18T12:40:55.466939Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Run ID (anthropic/claude-3-sonnet-20240229):  0195a945-7c09-7782-8e00-a82c9a6f496b\n",
      "Experiment Run ID (anthropic/claude-3-opus-20240229):  0195a945-7c9d-7d31-8276-2e59380b109d\n"
     ]
    }
   ],
   "execution_count": 43,
   "source": [
    "# 1. model: `anthropic/claude-3-sonnet`\n",
    "response = requests.post(\n",
    "    url=f'{GIP_CONSOLE_HOST}/api/workspaces/{GIP_WORKSPACE_ID}/evals/experiments/{dataset_experiment_id}/runs',\n",
    "    headers={'X-Gipc-Api-Key': GIP_CONSOLE_ENTERPRISE_API_KEY},\n",
    "    json={\n",
    "        'evaluator_id': category_metric_evaluator_id,\n",
    "        'display_name': 'Langfuse dataset + category metric experiment run + target data converter (anthropic/claude-3-sonnet-20240229)',\n",
    "        'target_data_converter': {\n",
    "            'model_seq_id': 6,\n",
    "            'model_public_id': 'anthropic/claude-3-sonnet-20240229',\n",
    "            'model_parameters': {\n",
    "                'top_p': 0.5,\n",
    "                'temperature': 0.6,\n",
    "                'max_tokens': 50,\n",
    "            },\n",
    "            'input_prompt_trajectory_template': {\n",
    "                'messages': [\n",
    "                    {\n",
    "                        'role': 'system',\n",
    "                        'content': 'You are an AI assistant that answers questions',\n",
    "                    },\n",
    "                    {\n",
    "                        'role': 'user',\n",
    "                        'content': '{item.input}',\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "response.raise_for_status()\n",
    "\n",
    "sonnet_experiment_run = response.json()\n",
    "sonnet_experiment_run_id = sonnet_experiment_run['id']\n",
    "\n",
    "# 2. model: `anthropic/claude-3-opus`\n",
    "response = requests.post(\n",
    "    url=f'{GIP_CONSOLE_HOST}/api/workspaces/{GIP_WORKSPACE_ID}/evals/experiments/{dataset_experiment_id}/runs',\n",
    "    headers={'X-Gipc-Api-Key': GIP_CONSOLE_ENTERPRISE_API_KEY},\n",
    "    json={\n",
    "        'evaluator_id': category_metric_evaluator_id,\n",
    "        'display_name': 'Langfuse dataset + category metric experiment run + target data converter (anthropic/claude-3-opus-20240229)',\n",
    "        'target_data_converter': {\n",
    "            'model_seq_id': 5,\n",
    "            'model_public_id': 'anthropic/claude-3-opus-20240229',\n",
    "            'model_parameters': {\n",
    "                'top_p': 0.5,\n",
    "                'temperature': 0.6,\n",
    "                'max_tokens': 50,\n",
    "            },\n",
    "            'input_prompt_trajectory_template': {\n",
    "                'messages': [\n",
    "                    {\n",
    "                        'role': 'system',\n",
    "                        'content': 'You are an AI assistant that answers questions',\n",
    "                    },\n",
    "                    {\n",
    "                        'role': 'user',\n",
    "                        'content': '{item.input}',\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "response.raise_for_status()\n",
    "\n",
    "opus_experiment_run = response.json()\n",
    "opus_experiment_run_id = opus_experiment_run['id']\n",
    "\n",
    "print('Experiment Run ID (anthropic/claude-3-sonnet-20240229): ', sonnet_experiment_run_id)\n",
    "print('Experiment Run ID (anthropic/claude-3-opus-20240229): ', opus_experiment_run_id)"
   ],
   "id": "6e61c33addf279de"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "위의 예시는 같은 Langfuse dataset의 데이터에 대해서 `anthropic/claude-3-sonnet`과 `anthropic/claude-3-opus` 모델의 성능을 비교하기 위한 experiment run을 생성한 것입니다. 각각의 결과를 통해서 모델간의 성능 차이를 확인할 수 있습니다.\n",
    "\n",
    "또한, experiment run 생성 요청에 나온 것처럼 `input_prompt_trajectory_template`를 통해서 각 모델에 대한 input prompt를 지정할 수 있습니다. system prompt를 변경하는 상황에 대한 비교가 필요한 경우에 이 점을 활용하는 것이 유용할 수 있습니다.\n",
    "\n",
    "추가로, `input_prompt_trajectory_template`의 값을 자세히 보면 `role: user`인 부분에 `{item.input}`이라는 템플릿이 사용된 것을 확인하실 수 있습니다. 이는 experiment에 지정된 데이터를 가져오는 과정에서 어떤 데이터를 가져올 수 있을지 지정하는 것입니다.\n",
    "\n",
    "예를 들어 experiment의 `type`이 `langfuse_dataset`인 경우, [DatasetItem](https://api.reference.langfuse.com/#tag/datasetitems/GET/api/public/dataset-items) 객체를 가져오게 됩니다. 해당 데이터에는 `input` 및 `expectedOutput`과 같은 속성 값들이 포함되어 있으며, 이를 활용하여 `input_prompt_trajectory_template`를 구성하는 것이 가능합니다. 만약에 `DatasetItem`의 `expectedOutput`을 사용하고 싶은 경우에는 `{item.input}`대신에 `{item.expectedOutput}` 표현을 사용하면 됩니다.\n",
    "\n",
    "experiment의 `type` 값에 따른 접근 가능한 객체는 다음과 같습니다.\n",
    "\n",
    "- `langfuse_dataset`: [DatasetItem](https://api.reference.langfuse.com/#tag/datasetitems/GET/api/public/dataset-items)\n",
    "- `langfuse_trace`: [TraceWithDetails](https://api.reference.langfuse.com/#tag/trace/GET/api/public/traces)\n",
    "- `langfuse_observation`: [ObservationsViews](https://api.reference.langfuse.com/#tag/observations/GET/api/public/observations)\n",
    "\n",
    "\n",
    "## Auto Evaluation\n",
    "\n",
    "GIP에서는 지정된 일정에 맞춰서 주기적으로 평가를 진행할 수 있도록 auto evaluation 기능을 지원합니다. auto evaluation을 위해서는 다음의 과정을 거쳐야 합니다.\n",
    "\n",
    "1. Langfuse credential 연동\n",
    "2. Experiment 생성: 평가 대상 지정\n",
    "3. Evaluator 생성: 평가 방법 지정\n",
    "4. Auto evaluation config 생성\n",
    "    - 평가 대상(experiment)에 대해서 어떻게 평가할지(evaluator)에 대한 정보의 조합으로 정해진 일정에 맞춰 experiment run을 생성하도록 지원합니다\n",
    "    - 지정된 일정에 따라 주기적으로 평가가 진행됩니다\n",
    "\n",
    "전반적인 과정은 앞서 살펴본 experiment run 생성과 동일하며, experiment run 생성을 auto evaluation config로 어느정도 대체하는 것으로 보는 것이 이해하시는 데에 도움이 될 수 있습니다.\n",
    "\n",
    "auto evaluation config를 생성하는 예시는 다음과 같습니다."
   ],
   "id": "cd780936f4e1e392"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "response = requests.post(\n",
    "    url=f'{GIP_CONSOLE_HOST}/api/workspaces/{GIP_WORKSPACE_ID}/evals/experiments/{dataset_experiment_id}/auto-evaluations',\n",
    "    headers={'X-Gipc-Api-Key': GIP_CONSOLE_ENTERPRISE_API_KEY},\n",
    "    json={\n",
    "        'evaluator_id': category_metric_evaluator_id,\n",
    "        'enabled': True,\n",
    "        'name': 'Langfuse dataset + category metric auto evaluation (auto)',\n",
    "        'schedule': '0 23 * * *', # cron expression: 매일 23시에 평가를 진행\n",
    "        'timezone': 'Asia/Seoul',  # default: `Etc/UTC`\n",
    "        'target_data_converter': None,\n",
    "        'metadata': None,\n",
    "    },\n",
    ")\n",
    "\n",
    "response.raise_for_status()\n",
    "\n",
    "basic_auto_evaluation_config = response.json()\n",
    "basic_auto_evaluation_config_id = basic_auto_evaluation_config['id']\n",
    "\n",
    "print('Auto Evaluation Config ID (basic): ', basic_auto_evaluation_config_id)"
   ],
   "id": "ccde2ca8b823dcfb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "위의 예시를 통해서 auto evaluation config를 생성한 것을 확인하실 수 있습니다. auto evaluation config를 생성하게 되면 지정된 일정(`schedule`, `timezone`)에 따라 experiment run이 생성되며, 자동으로 주기적인 평가가 진행됩니다.\n",
    "\n",
    "또한, experiment run 생성과 마찬가지로 `target_data_converter`를 지원하기 때문에 평가 대상 데이터를 변환하는 과정을 추가할 수 있습니다. 이를 통해서 평가 대상 데이터를 가공하여 자동으로 평가를 진행하는 것도 가능합니다."
   ],
   "id": "cde282682c4b4c16"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
